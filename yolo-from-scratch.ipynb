{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1126677,"sourceType":"datasetVersion","datasetId":388329},{"sourceId":4351695,"sourceType":"datasetVersion","datasetId":2561490},{"sourceId":5005548,"sourceType":"datasetVersion","datasetId":2899133},{"sourceId":159726,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":135790,"modelId":158520}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/yolo-from-scratch?scriptVersionId=211885320\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Class for training with custom dataset**","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics==8.2.103 -q\n\nfrom IPython import display\ndisplay.clear_output()\n\nimport ultralytics\nultralytics.checks()","metadata":{"execution":{"iopub.status.busy":"2024-11-08T08:13:13.93602Z","iopub.execute_input":"2024-11-08T08:13:13.936421Z","iopub.status.idle":"2024-11-08T08:13:47.796599Z","shell.execute_reply.started":"2024-11-08T08:13:13.936384Z","shell.execute_reply":"2024-11-08T08:13:47.795487Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.103 ðŸš€ Python-3.10.14 torch-2.4.0+cpu CPU (Intel Xeon 2.20GHz)\nSetup complete âœ… (4 CPUs, 31.4 GB RAM, 5933.9/8062.4 GB disk)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from ultralytics import YOLO\n\nfrom IPython.display import display, Image","metadata":{"execution":{"iopub.status.busy":"2024-11-08T08:13:51.594596Z","iopub.execute_input":"2024-11-08T08:13:51.595485Z","iopub.status.idle":"2024-11-08T08:13:51.600263Z","shell.execute_reply.started":"2024-11-08T08:13:51.595441Z","shell.execute_reply":"2024-11-08T08:13:51.599033Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nmodel_weights = torch.load(\"/kaggle/input/rifles_big_20_epochs/pytorch/default/1/best.pt\")\nprint(type(model_weights))\nfor k in model_weights: print(k)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T08:13:53.819639Z","iopub.execute_input":"2024-11-08T08:13:53.820512Z","iopub.status.idle":"2024-11-08T08:13:55.034167Z","shell.execute_reply.started":"2024-11-08T08:13:53.820468Z","shell.execute_reply":"2024-11-08T08:13:55.033099Z"},"trusted":true},"outputs":[{"name":"stdout","text":"<class 'dict'>\ndate\nversion\nlicense\ndocs\nepoch\nbest_fitness\nmodel\nema\nupdates\noptimizer\ntrain_args\ntrain_metrics\ntrain_results\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(f\"/kaggle/input/rifles_big_20_epochs/pytorch/default/1/best.pt\")  # load a custom model\n\nmodel.predict(\"/kaggle/input/weapon-data/weapon_data/images/val/im74.jpg\", save=True, imgsz=640, conf=0.5)\n\n# Validate the model\n#metrics = model.val()  # no arguments needed, dataset and settings remembered\n#metrics.box.map  # map50-95\n#metrics.box.map50  # map50\n#metrics.box.map75  # map75\n#metrics.box.maps  # a list contains map50-95 of each category","metadata":{"execution":{"iopub.status.busy":"2024-11-08T09:03:42.924866Z","iopub.execute_input":"2024-11-08T09:03:42.925718Z","iopub.status.idle":"2024-11-08T09:03:46.335538Z","shell.execute_reply.started":"2024-11-08T09:03:42.925673Z","shell.execute_reply":"2024-11-08T09:03:46.334539Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nimage 1/1 /kaggle/input/weapon-data/weapon_data/images/val/im74.jpg: 384x640 1 rifle, 1604.8ms\nSpeed: 2.6ms preprocess, 1604.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\nResults saved to \u001b[1mruns/detect/predict9\u001b[0m\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'rifle', 1: 'Shot gun', 2: 'Rifle', 3: 'rifle', 4: 'assaultrifle', 5: 'Rifle', 6: 'Rifle', 7: 'Rifle', 8: 'Rifle'}\n obb: None\n orig_img: array([[[ 76,  81,  80],\n         [ 86,  91,  90],\n         [ 94,  99,  98],\n         ...,\n         [137, 138, 134],\n         [137, 138, 134],\n         [137, 138, 134]],\n \n        [[ 76,  81,  80],\n         [ 86,  91,  90],\n         [ 94,  99,  98],\n         ...,\n         [135, 136, 132],\n         [136, 137, 133],\n         [136, 137, 133]],\n \n        [[ 77,  82,  81],\n         [ 87,  92,  91],\n         [ 94,  99,  98],\n         ...,\n         [135, 136, 132],\n         [136, 137, 133],\n         [136, 137, 133]],\n \n        ...,\n \n        [[147, 163, 156],\n         [146, 162, 155],\n         [144, 160, 153],\n         ...,\n         [130, 144, 143],\n         [130, 144, 143],\n         [129, 143, 142]],\n \n        [[143, 161, 154],\n         [142, 160, 153],\n         [155, 173, 166],\n         ...,\n         [132, 146, 145],\n         [131, 145, 144],\n         [131, 145, 144]],\n \n        [[142, 163, 155],\n         [140, 161, 153],\n         [158, 176, 169],\n         ...,\n         [134, 148, 147],\n         [133, 147, 146],\n         [132, 146, 145]]], dtype=uint8)\n orig_shape: (1079, 1919)\n path: '/kaggle/input/weapon-data/weapon_data/images/val/im74.jpg'\n probs: None\n save_dir: 'runs/detect/predict9'\n speed: {'preprocess': 2.5751590728759766, 'inference': 1604.8030853271484, 'postprocess': 0.9794235229492188}]"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(f\"/kaggle/input/transfer_learning_01.11/pytorch/default/1/best.pt\")  # load a custom model\n\nmodel.predict(\"/kaggle/input/test-weapon/valid/images/443443129-1612341199---Copy_jpeg.rf.4cc5357bfd132a63d43f45a704898a8b.jpg\", save=True, imgsz=640, conf=0.5, classes = [4])","metadata":{"execution":{"iopub.status.busy":"2024-11-02T10:45:41.156562Z","iopub.execute_input":"2024-11-02T10:45:41.157533Z","iopub.status.idle":"2024-11-02T10:45:44.817289Z","shell.execute_reply.started":"2024-11-02T10:45:41.15748Z","shell.execute_reply":"2024-11-02T10:45:44.816152Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nimage 1/1 /kaggle/input/test-weapon/valid/images/443443129-1612341199---Copy_jpeg.rf.4cc5357bfd132a63d43f45a704898a8b.jpg: 640x480 (no detections), 2004.3ms\nSpeed: 3.5ms preprocess, 2004.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\nResults saved to \u001b[1mruns/detect/predict3\u001b[0m\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'assaultrifle'}\n obb: None\n orig_img: array([[[248, 248, 248],\n         [248, 248, 248],\n         [248, 248, 248],\n         ...,\n         [190, 183, 180],\n         [208, 199, 196],\n         [210, 201, 198]],\n \n        [[248, 248, 248],\n         [248, 248, 248],\n         [248, 248, 248],\n         ...,\n         [218, 211, 208],\n         [234, 225, 222],\n         [225, 216, 213]],\n \n        [[248, 248, 248],\n         [248, 248, 248],\n         [248, 248, 248],\n         ...,\n         [239, 232, 229],\n         [246, 239, 236],\n         [224, 217, 214]],\n \n        ...,\n \n        [[163, 191, 208],\n         [147, 175, 192],\n         [134, 162, 179],\n         ...,\n         [135, 167, 186],\n         [137, 169, 188],\n         [135, 167, 186]],\n \n        [[143, 171, 188],\n         [136, 164, 181],\n         [132, 160, 177],\n         ...,\n         [125, 157, 176],\n         [127, 159, 178],\n         [126, 158, 177]],\n \n        [[123, 151, 168],\n         [123, 151, 168],\n         [126, 154, 171],\n         ...,\n         [126, 158, 177],\n         [126, 158, 177],\n         [124, 156, 175]]], dtype=uint8)\n orig_shape: (960, 720)\n path: '/kaggle/input/test-weapon/valid/images/443443129-1612341199---Copy_jpeg.rf.4cc5357bfd132a63d43f45a704898a8b.jpg'\n probs: None\n save_dir: 'runs/detect/predict3'\n speed: {'preprocess': 3.5257339477539062, 'inference': 2004.3418407440186, 'postprocess': 0.8707046508789062}]"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.box.map  # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps  # a list contains map50-95 of each category","metadata":{"execution":{"iopub.status.busy":"2024-11-02T10:45:55.526825Z","iopub.execute_input":"2024-11-02T10:45:55.527256Z","iopub.status.idle":"2024-11-02T11:54:31.852192Z","shell.execute_reply.started":"2024-11-02T10:45:55.527218Z","shell.execute_reply":"2024-11-02T11:54:31.850692Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.103 ðŸš€ Python-3.10.14 torch-2.4.0+cpu CPU (Intel Xeon 2.20GHz)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/weapon-detection/violence prediction in surveillance videos.v4i.yolov8/valid/labels... 1492 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1492/1492 [00:02<00:00, 556.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/weapon-detection/violence prediction in surveillance videos.v4i.yolov8/valid is not writeable, cache not saved.\nWARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 2, len(boxes) = 3026. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 94/94 [1:08:27<00:00, 43.69s/it]\n","output_type":"stream"},{"name":"stdout","text":"                   all       1492       3026      0.525      0.499      0.497       0.33\n               Grenade        132        277       0.79      0.682      0.746      0.574\n                 Knife        194        279      0.666      0.706      0.713      0.462\n                Pistol        520        635      0.786      0.735      0.788      0.556\n                 Rifle        829       1126      0.775      0.814      0.841      0.548\n             armed man        173        192      0.289      0.292      0.224      0.163\n                  body        161        216      0.477      0.347      0.399      0.212\n                  face        102        123      0.233      0.227      0.146     0.0788\n                  hand        130        178      0.187      0.191      0.115     0.0505\nSpeed: 4.3ms preprocess, 2737.7ms inference, 0.0ms loss, 1.2ms postprocess per image\nResults saved to \u001b[1mruns/detect/val3\u001b[0m\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([    0.57375,     0.46168,     0.33038,     0.55559,      0.5479,     0.16321,     0.21164,    0.078804,    0.050468])"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Load a pretrained YOLO11n model\nmodel = torch.load(\"/kaggle/input/test_detect/pytorch/default/1/best.pt\")\n\n# Run inference on an image\nresults = model(\"/kaggle/input/weapon-detection/violence prediction in surveillance videos.v4i.yolov8/test/images/2-307-_jpg.rf.d1a5c5a781e071d7aba32a8d0c74147e.jpg\")  # results list\n\n# View results\nfor r in results:\n    print(r.boxes)  # print the Boxes object containing the detection bounding boxes","metadata":{"execution":{"iopub.status.busy":"2024-10-30T08:33:56.664489Z","iopub.execute_input":"2024-10-30T08:33:56.667404Z","iopub.status.idle":"2024-10-30T08:33:57.046397Z","shell.execute_reply.started":"2024-10-30T08:33:56.66733Z","shell.execute_reply":"2024-10-30T08:33:57.0446Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nfrom IPython.display import Image, display\n# Model\n# model = torch.load(\"/kaggle/input/test_detect/pytorch/default/1/best.pt\")  # or yolov5m, yolov5l, yolov5x, etc.\nmodel = torch.hub.load('custom', '/kaggle/input/test_detect/pytorch/default/1/best.pt')  # custom trained model\n\n# Images\nim = '/kaggle/input/weapon-detection/violence prediction in surveillance videos.v4i.yolov8/test/images/160_jpg.rf.022c330ac8e9efd81980fc0bd47d87ac.jpg'  # or file, Path, URL, PIL, OpenCV, numpy, list\n\n# Inference\nresults = model(im)\n\n# Results\nresults.show()  # or .show(), .save(), .crop(), .pandas(), etc.\n\nresults.xyxy[0]  # im predictions (tensor)\nresults.pandas().xyxy[0]  # im predictions (pandas)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T08:23:56.27261Z","iopub.execute_input":"2024-10-30T08:23:56.273835Z","iopub.status.idle":"2024-10-30T08:23:56.386906Z","shell.execute_reply.started":"2024-10-30T08:23:56.273789Z","shell.execute_reply":"2024-10-30T08:23:56.385072Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = torch.load('/kaggle/input/test_detect/pytorch/default/1/best.pt')\nlast_key = list(model['model'].values())[-1]\nkey_length = list(map(model['model'].get, last_key))\nprint((key_length))","metadata":{"execution":{"iopub.status.busy":"2024-10-30T07:30:17.534184Z","iopub.execute_input":"2024-10-30T07:30:17.534698Z","iopub.status.idle":"2024-10-30T07:30:18.040181Z","shell.execute_reply.started":"2024-10-30T07:30:17.534652Z","shell.execute_reply":"2024-10-30T07:30:18.038565Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/input/test_detect/pytorch/default/1/best.pt'))","metadata":{"execution":{"iopub.status.busy":"2024-10-30T07:55:32.811622Z","iopub.execute_input":"2024-10-30T07:55:32.812088Z","iopub.status.idle":"2024-10-30T07:55:32.850944Z","shell.execute_reply.started":"2024-10-30T07:55:32.812045Z","shell.execute_reply":"2024-10-30T07:55:32.849002Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = torch.load('/kaggle/input/test_detect/pytorch/default/1/best.pt')\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T07:41:14.497339Z","iopub.execute_input":"2024-10-30T07:41:14.498954Z","iopub.status.idle":"2024-10-30T07:41:14.765227Z","shell.execute_reply.started":"2024-10-30T07:41:14.498894Z","shell.execute_reply":"2024-10-30T07:41:14.763826Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PATH = '/kaggle/input/test_detect/pytorch/default/1/best.pt'\nstate = {'model': model.state_dict()}\ntorch.save(state, PATH)\nmodel.load_state_dict(torch.load(PATH)['model'])\n# print weights\nfor k, v in model.named_parameters():\n    print(k, v)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T07:48:32.404566Z","iopub.execute_input":"2024-10-30T07:48:32.405635Z","iopub.status.idle":"2024-10-30T07:48:32.454722Z","shell.execute_reply.started":"2024-10-30T07:48:32.405578Z","shell.execute_reply":"2024-10-30T07:48:32.452786Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\ndef rename_and_copy_images(source_folder, dest_folder, prefix):\n    for filename in os.listdir(source_folder):\n        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n            new_name = f\"{prefix}_{filename}\"\n            shutil.copy(os.path.join(source_folder, filename), os.path.join(dest_folder, new_name))\n\nrename_and_copy_images(\"/kaggle/input/weapon-detection/Weapon Detection.v3i.yolov8/train/images\", \"/kaggle/working\", \"ds1\")\nrename_and_copy_images(\"/kaggle/input/weapon-detection/violence prediction in surveillance videos.v4i.yolov8/train/images\", \"/kaggle/working\", \"ds2\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T14:53:52.434731Z","iopub.execute_input":"2024-10-29T14:53:52.435146Z","iopub.status.idle":"2024-10-29T14:54:46.506758Z","shell.execute_reply.started":"2024-10-29T14:53:52.43511Z","shell.execute_reply":"2024-10-29T14:54:46.505794Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def adjust_class_labels(annotation_file, class_mapping):\n    with open(annotation_file, 'r') as f:\n        lines = f.readlines()\n\n    with open(annotation_file, 'w') as f:\n        for line in lines:\n            parts = line.strip().split()\n            old_class_id = int(parts[0])\n            new_class_id = class_mapping.get(old_class_id, old_class_id)\n            f.write(f\"{new_class_id} \" + \" \".join(parts[1:]) + \"\\n\")\n\n# Adjust class labels for each .txt annotation file in dataset\nclass_mapping = {0: 1, 1: 0}  # Example mapping\nfor file in os.listdir(\"/kaggle/working/merged_dataset/images/train\"):\n    adjust_class_labels(os.path.join(\"/kaggle/working/merged_dataset/images/train\", file), class_mapping)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport cv2\nimport numpy as np\n\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, annotations, transforms=None):\n        self.image_paths = image_paths\n        self.annotations = annotations\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        # Load image\n        image = cv2.imread(self.image_paths[index])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Get annotations\n        boxes = self.annotations[index]['boxes'] # Example: assuming annotations contain boxes\n        \n        # Apply custom augmentations\n        if self.transforms:\n            augmented = self.transforms(image=image, bboxes=boxes)\n            image = augmented['image']\n            boxes = augmented['bboxes']\n\n        # Convert everything into a torch.Tensor\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        boxes = np.array(boxes).astype(np.float32)\n        \n        return {'image': image, 'bboxes': boxes}\n\n# Example: Assuming you have lists of image paths and annotations\nimage_paths = ['path/to/image1.jpg', 'path/to/image2.jpg']\nannotations = [{'boxes': [[10, 20, 30, 40]]}, {'boxes': [[50, 60, 70, 80]]}] # Example format\n\n# Optionally include transformations\ntransforms = None \n\n# Instantiate the dataset\ncustom_dataset = CustomDataset(image_paths=image_paths, annotations=annotations, transforms=transforms)\n\n# Use this custom dataset for training with YOLOv8","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Dataloader with albumentations**","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# Define your Albumentations transformations\ntransforms = A.Compose([\n    A.Resize(640, 640),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n    ToTensorV2()\n], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n\n# Custom dataset class using Albumentations\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, annotations):\n        self.image_paths = image_paths\n        self.annotations = annotations\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        boxes = self.annotations[idx]['boxes']\n        image = cv2.imread(image_path)\n        transformed = transforms(image=image, bboxes=boxes, class_labels=[0]*len(boxes))\n        return transformed['image'], transformed['bboxes']\n\n    def __len__(self):\n        return len(self.image_paths)\n\n# Initialize and load your dataset and dataloader\ndataset = CustomDataset(image_paths, annotations)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n# Set up and train your model\nmodel = YOLO('yolov8n.pt')\nfor epoch in range(epochs):\n    for images, targets in dataloader:\n        results = model.train(images, targets)  # Adapt this pseudocode for proper API integration","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training with albumentations**","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\nfrom your_custom_dataloader import CustomDataset, DataLoader\n\n# Create DataLoader with Albumentations\ndataset = CustomDataset(image_paths, annotations, transforms=your_albumentations_transforms)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n# Load model\nmodel = YOLO('yolov8n.pt')\n\n# Training loop\nfor images, targets in dataloader:\n    loss = model.train_step(images, targets)  # Pseudocode for illustration","metadata":{},"outputs":[],"execution_count":null}]}